# 最小平方回歸 (Least Squares Regression)

> 當 Ax = b 無解時，找「最接近的解」——機器學習的基石

## 1. 問題背景

### 超定系統（Overdetermined System）

當方程式數量 > 未知數數量時，通常**無解**。

```
例：3 個方程，2 個未知數

x + y = 1
x - y = 1
2x + y = 3

矩陣形式：Ax = b

[1  1] [x]   [1]
[1 -1] [y] = [1]
[2  1]       [3]
```

這個系統無解（三條直線不交於一點），但我們可以找**最佳近似解**。

### 為什麼這很重要？

| 應用 | 說明 |
|------|------|
| **線性迴歸** | 機器學習最基本的模型 |
| **曲線擬合** | 從數據點找最佳曲線 |
| **統計分析** | 找趨勢線 |
| **信號處理** | 濾波、去噪 |
| **科學實驗** | 分析實驗數據 |

---

## 2. 最小平方的定義

### 目標

找 x̂ 使**殘差向量的長度最小**：

```
最小化 ‖Ax - b‖²
```

### 幾何意義

Ax̂ 是 b 在 C(A)（行空間）上的**投影**。

```
     b
     ↑
     |  \
     |   \  e = b - Ax̂（殘差）
     |    \
     |     ↓
     +-----→ Ax̂ = p（投影）
        C(A)
```

---

## 3. 正規方程（Normal Equation）

### 推導

因為 e = b - Ax̂ 垂直於 C(A)，所以：

```
Aᵀe = 0
Aᵀ(b - Ax̂) = 0
Aᵀb - AᵀAx̂ = 0
```

### 正規方程

```
AᵀA x̂ = Aᵀb
```

### 解

若 AᵀA 可逆（A 的行向量線性獨立）：

```
x̂ = (AᵀA)⁻¹ Aᵀb
```

### 投影

```
p = Ax̂ = A(AᵀA)⁻¹Aᵀb
```

---

## 4. 簡單線性迴歸

### 問題

給定 n 個數據點 (t₁, b₁), (t₂, b₂), ..., (tₙ, bₙ)，
找直線 y = C + Dt 最佳擬合這些點。

### 設計矩陣

```
    [1  t₁]       [b₁]
A = [1  t₂]   b = [b₂]
    [⋮  ⋮ ]       [⋮ ]
    [1  tₙ]       [bₙ]

    [C]
x = [D]
```

### 正規方程

```
AᵀA = [n      Σtᵢ  ]
      [Σtᵢ   Σtᵢ² ]

Aᵀb = [Σbᵢ   ]
      [Σtᵢbᵢ ]
```

### 解

```
AᵀA x̂ = Aᵀb

[n      Σtᵢ  ] [C]   [Σbᵢ   ]
[Σtᵢ   Σtᵢ² ] [D] = [Σtᵢbᵢ ]
```

---

## 5. 完整範例

### 數據

| t | b |
|---|---|
| 0 | 1 |
| 1 | 3 |
| 2 | 4 |

找最佳直線 y = C + Dt

### 設計矩陣

```
    [1  0]       [1]
A = [1  1]   b = [3]
    [1  2]       [4]
```

### 計算 AᵀA 和 Aᵀb

```
AᵀA = [1 1 1] [1  0]   [3  3]
      [0 1 2] [1  1] = [3  5]
              [1  2]

Aᵀb = [1 1 1] [1]   [8]
      [0 1 2] [3] = [11]
              [4]
```

### 解正規方程

```
[3  3] [C]   [8]
[3  5] [D] = [11]

使用克拉瑪法則或高斯消去：
det(AᵀA) = 15 - 9 = 6

C = (8×5 - 11×3)/6 = (40 - 33)/6 = 7/6 ≈ 1.167
D = (3×11 - 3×8)/6 = (33 - 24)/6 = 9/6 = 1.5
```

### 結果

```
最佳直線：y = 1.167 + 1.5t

驗證：
t=0: y = 1.167（實際 1）
t=1: y = 2.667（實際 3）
t=2: y = 4.167（實際 4）
```

---

## 6. 多項式擬合

### 問題

用 k 次多項式擬合數據：
```
y = c₀ + c₁t + c₂t² + ... + cₖtᵏ
```

### 設計矩陣（Vandermonde 矩陣）

```
    [1  t₁  t₁²  ...  t₁ᵏ]
A = [1  t₂  t₂²  ...  t₂ᵏ]
    [⋮   ⋮   ⋮   ⋱   ⋮  ]
    [1  tₙ  tₙ²  ...  tₙᵏ]
```

---

## 7. 殘差分析

### 殘差向量

```
e = b - Ax̂ = b - p
```

### 殘差平方和（RSS）

```
RSS = ‖e‖² = Σ(bᵢ - ŷᵢ)²
```

### 決定係數（R²）

```
R² = 1 - RSS/TSS

其中 TSS = Σ(bᵢ - b̄)²（總平方和）
```

R² 越接近 1，擬合越好。

---

## 8. 數值穩定性

### 問題

當 AᵀA 接近奇異（條件數大）時，直接求逆會有數值問題。

### 解決方案

1. **QR 分解**：A = QR，則 Rx̂ = Qᵀb
2. **SVD**：最穩健但較慢
3. **正則化**：嶺迴歸（Ridge Regression）

### QR 分解的優勢

```
原本：(AᵀA)x̂ = Aᵀb
      (QR)ᵀ(QR)x̂ = (QR)ᵀb
      RᵀQᵀQRx̂ = RᵀQᵀb
      RᵀRx̂ = RᵀQᵀb
      Rx̂ = Qᵀb（因為 R 可逆）

直接回代求解，避免計算 AᵀA
```

---

## 9. 加權最小平方

### 問題

不同數據點有不同的可信度。

### 加權正規方程

```
最小化 Σ wᵢ(bᵢ - aᵢᵀx)²

正規方程：AᵀWAx̂ = AᵀWb

其中 W = diag(w₁, w₂, ..., wₙ)
```

---

## 10. 程式實作

### 本單元程式示範

| 檔案 | 內容 |
|------|------|
| `least_squares_manual.py` | 手刻正規方程 |
| `least_squares_numpy.py` | 使用 NumPy 和 QR 分解 |

---

## 11. 考試重點

### 常見題型

1. **建立設計矩陣**：給數據寫出 A 和 b
2. **解正規方程**：計算 AᵀA 和 Aᵀb，求解
3. **計算殘差**：求 ‖e‖
4. **幾何解釋**：說明投影的意義
5. **證明正規方程**：從 Aᵀe = 0 推導

### 必背公式

```
正規方程：AᵀA x̂ = Aᵀb
最佳解：x̂ = (AᵀA)⁻¹Aᵀb
投影：p = A(AᵀA)⁻¹Aᵀb
殘差：e = b - Ax̂
```

### 常見錯誤

- 混淆 A 和 Aᵀ 的位置
- 忘記檢查 AᵀA 是否可逆
- 設計矩陣寫錯（忘記常數項的 1）

---

## 12. 與機器學習的連結

### 線性迴歸

最小平方法就是**線性迴歸**的數學基礎。

### 損失函數

```
Loss = ‖Ax - b‖² = (Ax - b)ᵀ(Ax - b)
```

### 梯度下降

```
∇Loss = 2Aᵀ(Ax - b)

設 ∇Loss = 0，得到正規方程 AᵀAx = Aᵀb
```

### 正則化

- **L2 正則化（嶺迴歸）**：(AᵀA + λI)x̂ = Aᵀb
- **L1 正則化（Lasso）**：需要優化方法

---

## 參考資料

- Strang, Chapter 4.3: Least Squares Approximations
- MIT 18.06 Lecture 16
- Bishop, Pattern Recognition and Machine Learning, Chapter 3
